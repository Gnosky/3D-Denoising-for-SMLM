{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "    - Test that rotation is correct\n",
    "    - decide how to compute sample statistics (scalar v mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T20:23:08.545863Z",
     "start_time": "2019-11-02T20:23:05.702205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Conv2D, LeakyReLU, UpSampling2D, MaxPooling2D, ZeroPadding2D, Cropping2D, Concatenate, Reshape, GlobalAveragePooling2D\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Conv3D, UpSampling3D, MaxPooling3D, ZeroPadding3D, Cropping3D\n",
    "import imageio\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt 2-d auxiliary functions to 3-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T20:23:08.589150Z",
     "start_time": "2019-11-02T20:23:08.577153Z"
    }
   },
   "outputs": [],
   "source": [
    "def _vshifted_conv(x, num_filters, name):\n",
    "    \"\"\" \n",
    "    Vertically shifted 3-d convolution\n",
    "    \"\"\"\n",
    "    filter_size = [3,3,3]\n",
    "    # Assumes the height is the second dimension\n",
    "    k = filter_size[1]//2\n",
    "\n",
    "    ### 2d code ###\n",
    "#     x = ZeroPadding2D([[k,0],[0,0]])(x)\n",
    "#     x = Conv2D(filters=num_filters, kernel_size=filter_size, padding='same', kernel_initializer='he_normal', name=name)(x)\n",
    "#     x = LeakyReLU(0.1)(x)\n",
    "#     x = Cropping2D([[0,k],[0,0]])(x)\n",
    "\n",
    "    ### 3d adaptation ###\n",
    "    \n",
    "    # assumes first tuple is frame number, second is height, 3rd is width\n",
    "    # padding on height\n",
    "    x = ZeroPadding3D([[0,0],[k,0],[0,0]])(x)\n",
    "    x = Conv3D(filters=num_filters, kernel_size=filter_size, padding='same', kernel_initializer='he_normal', name=name)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Cropping3D([[0,0],[0,k],[0,0]])(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T20:23:08.601149Z",
     "start_time": "2019-11-02T20:23:08.593149Z"
    }
   },
   "outputs": [],
   "source": [
    "def _vshifted_pool(x):\n",
    "    \"\"\" \n",
    "    Vertically shifted max pooling 3d\n",
    "    \"\"\"\n",
    "    \n",
    "    ### 2d code ###\n",
    "#     x = ZeroPadding2D([[1,0],[0,0]])(x)\n",
    "#     x = Cropping2D([[0,1],[0,0]])(x)\n",
    "\n",
    "#     x = MaxPooling2D(pool_size=2,strides=2,padding='same')(x)\n",
    "\n",
    "    ### 3d adaptation ###\n",
    "    x = ZeroPadding3D([[0,0],[1,0],[0,0]])(x)\n",
    "    x = Cropping3D([[0,0],[0,1],[0,0]])(x)\n",
    "    \n",
    "    x = MaxPooling3D(pool_size=(2,2,2),strides=2,padding='same')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T20:23:08.621195Z",
     "start_time": "2019-11-02T20:23:08.605152Z"
    }
   },
   "outputs": [],
   "source": [
    "def _vertical_blindspot_network(x):\n",
    "    \"\"\" Blind-spot network; adapted from noise2noise GitHub\n",
    "    Each row of output only sees input pixels above that row\n",
    "    \"\"\"\n",
    "    skips = [x]\n",
    "\n",
    "    n = x\n",
    "    n = _vshifted_conv(n, 48, 'enc_conv0')\n",
    "    n = _vshifted_conv(n, 48, 'enc_conv1')\n",
    "    n = _vshifted_pool(n)\n",
    "\n",
    "    skips.append(n)\n",
    "\n",
    "    n = _vshifted_conv(n, 48, 'enc_conv2')\n",
    "    n = _vshifted_pool(n)\n",
    "    \n",
    "    skips.append(n)\n",
    "\n",
    "    n = _vshifted_conv(n, 48, 'enc_conv3')\n",
    "    n = _vshifted_pool(n)\n",
    "    \n",
    "    skips.append(n)\n",
    "\n",
    "    n = _vshifted_conv(n, 48, 'enc_conv4')\n",
    "    n = _vshifted_pool(n)\n",
    "    \n",
    "    skips.append(n)\n",
    "\n",
    "    n = _vshifted_conv(n, 48, 'enc_conv5')\n",
    "    n = _vshifted_pool(n)\n",
    "    n = _vshifted_conv(n, 48, 'enc_conv6')\n",
    "\n",
    "    #-----------------------------------------------\n",
    "    n = UpSampling3D(2)(n)\n",
    "\n",
    "    n = Concatenate(axis=4)([n, skips.pop()])\n",
    "    n = _vshifted_conv(n, 96, 'dec_conv5')\n",
    "    n = _vshifted_conv(n, 96, 'dec_conv5b')\n",
    "\n",
    "    n = UpSampling3D(2)(n)\n",
    "\n",
    "    n = Concatenate(axis=4)([n, skips.pop()])\n",
    "    n = _vshifted_conv(n, 96, 'dec_conv4')\n",
    "    n = _vshifted_conv(n, 96, 'dec_conv4b')\n",
    "\n",
    "    n = UpSampling3D(2)(n)\n",
    "    n = Concatenate(axis=4)([n, skips.pop()])\n",
    "    n = _vshifted_conv(n, 96, 'dec_conv3')\n",
    "    n = _vshifted_conv(n, 96, 'dec_conv3b')\n",
    "\n",
    "    n = UpSampling3D(2)(n)\n",
    "    n = Concatenate(axis=4)([n, skips.pop()])\n",
    "    n = _vshifted_conv(n, 96, 'dec_conv2')\n",
    "    n = _vshifted_conv(n, 96, 'dec_conv2b')\n",
    "\n",
    "    n = UpSampling3D(2)(n)\n",
    "    n = Concatenate(axis=4)([n, skips.pop()])\n",
    "    n = _vshifted_conv(n, 96, 'dec_conv1a')\n",
    "    n = _vshifted_conv(n, 96, 'dec_conv1b')\n",
    "\n",
    "    # final pad and crop for blind spot\n",
    "    n = ZeroPadding3D([[0,0],[1,0],[0,0]])(n)\n",
    "    n = Cropping3D([[0,0],[0,1],[0,0]])(n)\n",
    "\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T20:43:59.363970Z",
     "start_time": "2019-11-02T20:43:59.347659Z"
    }
   },
   "outputs": [],
   "source": [
    "def blindspot_network(inputs):\n",
    "    # batch, height, width, depth, channel\n",
    "    b,d,h,w,c = K.int_shape(inputs)\n",
    "    #if h != w:\n",
    "    #raise ValueError('input shape must be square')\n",
    "    if h % 32 != 0 or w % 32 != 0 or d % 32 != 0:\n",
    "        raise ValueError('input shape (%d x %d x %d) must be divisible by 32'%(h,w,d))\n",
    "\n",
    "    # make vertical blindspot network\n",
    "    vert_input = Input([h,w,d,c])\n",
    "    vert_output = _vertical_blindspot_network(vert_input)\n",
    "    vert_model = Model(inputs=vert_input,outputs=vert_output)\n",
    "        \n",
    "    # run vertical blindspot network on rotated inputs\n",
    "    stacks = []\n",
    "    for i in range(4):\n",
    "        # Rotate along width prior to network being run\n",
    "        rotated = inputs\n",
    "        for j in range(i):\n",
    "            rotated = Lambda(lambda x: tf.transpose(x, perm = [0,2,1,3,4]))(rotated)\n",
    "            rotated = Lambda(lambda x: tf.reverse(x, axis = [1]))(rotated)\n",
    "\n",
    "        if i == 0 or i == 2:\n",
    "            rotated = Reshape([d,w,h,c])(rotated)\n",
    "        else:\n",
    "            rotated = Reshape([d,h,w,c])(rotated)\n",
    "        out = vert_model(rotated)\n",
    "        \n",
    "        # Undo the rotation after the network is run\n",
    "        for j in range(i):\n",
    "            out = Lambda(lambda x: tf.transpose(tf.reverse(x, axis = [1]),perm = [0,2,1,3,4]))(out)\n",
    "            \n",
    "        stacks.append(out)\n",
    "        \n",
    "    for i in [1,3]:\n",
    "        rotated = inputs\n",
    "        # Rotate along depth axis prior to network being run\n",
    "        for j in range(i):\n",
    "            rotated = Lambda(lambda x: tf.transpose(x, perm = [0,1,3,2,4]))(rotated) \n",
    "            rotated = Lambda(lambda x: tf.reverse(x, axis = [3]))(rotated)\n",
    "        out = vert_model(rotated)\n",
    "        \n",
    "        # Undo rotation after network is run\n",
    "        for j in range(i):\n",
    "            out = Lambda(lambda x: tf.transpose(tf.reverse(x, axis = [3]),perm = [0,1,3,2,4]))(out)\n",
    "        stacks.append(out)\n",
    "\n",
    "    stacks = [vert_model(inputs) for i in range(6)]\n",
    "    # concatenate outputs\n",
    "    x = Concatenate(axis=4)(stacks)\n",
    "\n",
    "    # final 1x1 convolutional layers\n",
    "    x = Conv3D(384, 1, kernel_initializer='he_normal', name='conv1x1_1')(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "\n",
    "    x = Conv3D(96, 1, kernel_initializer='he_normal', name='conv1x1_2')(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T20:23:08.653151Z",
     "start_time": "2019-11-02T20:23:08.637150Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error_loss(y,loc):\n",
    "    return K.mean(0.5*K.pow(y-loc,2))\n",
    "\n",
    "def mse_blindspot_network(input_shape,train_mean=0,train_std=1):\n",
    "    # create input layer\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # apply normalization\n",
    "    norm_input = Lambda(lambda x: (x-train_mean)/train_std)(inputs)\n",
    "\n",
    "    # run blindspot network\n",
    "    x = blindspot_network(norm_input)\n",
    "\n",
    "    loc = Conv3D(1, 1, name='loc')(x)\n",
    "\n",
    "    output = Lambda(lambda x: x*train_std+train_mean)(loc)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=inputs,outputs=output)\n",
    "\n",
    "    # create loss function\n",
    "    loss = mean_squared_error_loss(norm_input,loc)\n",
    "    model.add_loss(loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a training scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T20:23:08.757190Z",
     "start_time": "2019-11-02T20:23:08.657150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data shape (100, 703, 515)\n",
      "train data shape (70, 128, 128)\n",
      "validation data shape (30, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "path = \"12_first_100_frames_YFP.tif\"\n",
    "\n",
    "data = imageio.volread(path)\n",
    "print(\"total data shape\", data.shape)\n",
    "\n",
    "# Start with square crops for simplicity\n",
    "train_images = data[:70,128:256,128:256]\n",
    "val_images = data[70:,128:256,128:256]\n",
    "print(\"train data shape\", train_images.shape)\n",
    "print(\"validation data shape\", val_images.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following train.py from poisson denoising/FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T20:23:08.793150Z",
     "start_time": "2019-11-02T20:23:08.757190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_train_imgs shape (70, 128, 128, 1)\n",
      "np_val_imgs shape (30, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The images are 8-bit (0-255 range) so we convert them to floating point, 0-1 range.\"\"\"\n",
    "\n",
    "norm = lambda im : (im / 255.0).reshape((128, 128, 1))\n",
    "np_train_imgs = np.array([norm(im) for im in train_images])\n",
    "np_val_imgs = np.array([norm(im) for im in val_images])\n",
    "\n",
    "print(\"np_train_imgs shape\", np_train_imgs.shape)\n",
    "print(\"np_val_imgs shape\", np_val_imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate sample statistics from the images. Might want to consider using mean for each image instead of mean overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T20:23:08.829149Z",
     "start_time": "2019-11-02T20:23:08.793150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.760540942451195\n",
      "9.05726232473462\n"
     ]
    }
   ],
   "source": [
    "train_mean = np.mean(np_train_imgs)\n",
    "train_std = np.std(np_train_imgs)\n",
    "print(train_mean)\n",
    "print(train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T20:44:06.088571Z",
     "start_time": "2019-11-02T20:44:03.807079Z"
    }
   },
   "outputs": [],
   "source": [
    "model = mse_blindspot_network((32,32,32,1), train_mean, train_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
